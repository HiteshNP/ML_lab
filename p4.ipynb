{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim:** To understand the working principle of Artificial Neural Network with feed forward and feed backward principle.\n",
    "\n",
    "**Program:** Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted Output: \n",
      " [[0.89437395]\n",
      " [0.88245813]\n",
      " [0.8931208 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 0.2\n",
    "input_neurons = 2\n",
    "hidden_neurons = 3\n",
    "output_neurons = 1\n",
    "\n",
    "weights_input_hidden = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
    "bias_hidden = np.random.uniform(size=(1, hidden_neurons))\n",
    "weights_hidden_output = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
    "bias_output = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    error_output_layer = y - predicted_output\n",
    "    gradient_output_layer = sigmoid_gradient(predicted_output)\n",
    "    delta_output_layer = error_output_layer * gradient_output_layer\n",
    "\n",
    "    error_hidden_layer = np.dot(delta_output_layer, weights_hidden_output.T)\n",
    "    gradient_hidden_layer = sigmoid_gradient(hidden_layer_output)\n",
    "    delta_hidden_layer = error_hidden_layer * gradient_hidden_layer\n",
    "\n",
    "    weights_hidden_output += np.dot(hidden_layer_output.T, delta_output_layer) * learning_rate\n",
    "    weights_input_hidden += np.dot(X.T, delta_hidden_layer) * learning_rate\n",
    "\n",
    "print(\"Normalized Input: \\n\" , str(X))\n",
    "print(\"\\nActual Output: \\n\" , str(y))\n",
    "print(\"\\nPredicted Output: \\n\" , str(predicted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the purpose of normalizing the input and output data?\n",
    "\n",
    "Normalizing the data ensures that all input features and output values are on a similar scale, which helps in faster and more stable convergence during training.\n",
    "\n",
    "### 2. Why is the sigmoid function used as an activation function?\n",
    "\n",
    "The sigmoid function squashes inputs to a range between 0 and 1, which is useful for modeling probabilities and makes the gradient descent algorithm work efficiently.\n",
    "\n",
    "### 3. What role does the gradient of the sigmoid function play in backpropagation?\n",
    "\n",
    "The gradient of the sigmoid function is used to adjust the weights during backpropagation by calculating how much each weight contributed to the error.\n",
    "\n",
    "### 4. How are the weights updated during training?\n",
    "\n",
    "Weights are updated by calculating the gradient of the error with respect to each weight and then adjusting the weights by a fraction of this gradient, controlled by the learning rate.\n",
    "\n",
    "### 5. What are the main components of a neural network's forward and backward pass?\n",
    "\n",
    "The forward pass involves calculating activations from input to output through each layer. The backward pass involves computing gradients and updating weights to minimize the error.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
