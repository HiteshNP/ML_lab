{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim:** To construct the Decision tree using the training data sets under supervised learning concept.\n",
    "\n",
    "**Program:** Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Outlook', 'Temperature', 'Humidity', 'Wind', 'PlayTennis']\n",
      "['sunny', 'hot', 'high', 'weak', 'no']\n",
      "['sunny', 'hot', 'high', 'strong', 'no']\n",
      "['overcast', 'hot', 'high', 'weak', 'yes']\n",
      "['rain', 'mild', 'high', 'weak', 'yes']\n",
      "['rain', 'cool', 'normal', 'weak', 'yes']\n",
      "['rain', 'cool', 'normal', 'strong', 'no']\n",
      "['overcast', 'cool', 'normal', 'strong', 'yes']\n",
      "['sunny', 'mild', 'high', 'weak', 'no']\n",
      "['sunny', 'cool', 'normal', 'weak', 'yes']\n",
      "['rain', 'mild', 'normal', 'weak', 'yes']\n",
      "['sunny', 'mild', 'normal', 'strong', 'yes']\n",
      "['overcast', 'mild', 'high', 'strong', 'yes']\n",
      "['overcast', 'hot', 'normal', 'weak', 'yes']\n",
      "['rain', 'mild', 'high', 'strong', 'no']\n",
      "\n",
      "Decision Tree: {'Outlook': {'rain': {'Wind': {'weak': 'yes', 'strong': 'no'}}, 'overcast': 'yes', 'sunny': {'Humidity': {'normal': 'yes', 'high': 'no'}}}}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from math import log2\n",
    "\n",
    "def get_data(file):\n",
    "    with open(file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        data = list(csv_reader)\n",
    "        for line in data:\n",
    "            print(line)\n",
    "    return data[1:], data[0]\n",
    "\n",
    "def entropy(data):\n",
    "    outcomes = [row[-1] for row in data]\n",
    "    probs = [outcomes.count(value) / len(outcomes) for value in set(outcomes)]\n",
    "    return -sum(p * log2(p) for p in probs)\n",
    "\n",
    "def split_data(data, attr):\n",
    "    values = set(row[attr] for row in data)\n",
    "    return [[row for row in data if row[attr] == value] for value in values]\n",
    "\n",
    "def best_attribute(data):\n",
    "    base_entropy = entropy(data)\n",
    "    gain = [(base_entropy - sum((len(subset) / len(data)) * entropy(subset) for subset in split_data(data, attr)), attr) \n",
    "            for attr in range(len(data[0]) - 1)]\n",
    "    return max(gain)[1]\n",
    "\n",
    "def decision_tree(data, labels):\n",
    "    outcomes = [row[-1] for row in data]\n",
    "    if outcomes.count(outcomes[0]) == len(outcomes):\n",
    "        return outcomes[0]\n",
    "    \n",
    "    attr = best_attribute(data)\n",
    "    tree = {labels[attr]: {}}\n",
    "\n",
    "    for value in set(row[attr] for row in data):\n",
    "        sub_labels = labels[:attr] + labels[attr+1:]\n",
    "        sub_data = [row[:attr] + row[attr+1:] for row in data if row[attr] == value]\n",
    "\n",
    "        tree[labels[attr]][value] = decision_tree(sub_data, sub_labels)\n",
    "        \n",
    "    return tree\n",
    "\n",
    "data, labels = get_data(\"id3.csv\")\n",
    "tree = decision_tree(data, labels)\n",
    "print(\"\\nDecision Tree:\", tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the ID3 algorithm?\n",
    "\n",
    "The ID3 algorithm is a decision tree learning algorithm that uses entropy and information gain to select the best attribute for splitting the data at each node.\n",
    "\n",
    "### 2. How is entropy calculated in this code?\n",
    "\n",
    "Entropy is calculated by summing up `-p * log2(p)` for each probability `p` of the unique outcomes in the dataset, representing the uncertainty in the data.\n",
    "\n",
    "### 3. What does information gain represent in decision tree algorithms?\n",
    "\n",
    "Information gain measures the reduction in entropy or uncertainty when a dataset is split on an attribute. It helps in choosing the attribute that best separates the data.\n",
    "\n",
    "### 4. Why does the `decision_tree` function check if all outcomes are the same before splitting?\n",
    "\n",
    "If all outcomes are the same, it means that the data is pure, and there's no need for further splitting, allowing the function to return a leaf node with that outcome.\n",
    "\n",
    "### 5. What happens when the dataset is split on an attribute?\n",
    "\n",
    "When the dataset is split on an attribute, it is divided into subsets, each containing rows where the attribute has the same value. The decision tree then branches accordingly.\n",
    "\n",
    "### 6. Why do we remove an attribute from the label list after splitting the data on it?\n",
    "\n",
    "We remove the attribute to avoid re-splitting the data on the same attribute, ensuring that the decision tree progresses and does not revisit the same decision point.\n",
    "\n",
    "### 7. What are the limitations of the ID3 algorithm?\n",
    "\n",
    "The ID3 algorithm tends to favor attributes with more levels (values), which may lead to overfitting. It also assumes that the data is categorical and does not handle continuous attributes without modification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
